{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KAHVoZ2O3x_2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import operator\n",
    "main_path = '/content/drive/My Drive/Colab Notebooks/'\n",
    "question = np.load(main_path + 'middle_data/' + 'pad_question.npy')\n",
    "answer = np.load(main_path + 'middle_data/' + 'pad_answer.npy')\n",
    "answer_o = np.load(main_path + 'middle_data/' + 'answer_o.npy', allow_pickle=True)\n",
    "with open(main_path + 'middle_data/' + 'vocab_bag.pkl', 'rb') as f:\n",
    "    words = pickle.load(f)\n",
    "with open(main_path + 'middle_data/' + 'pad_word_to_index.pkl', 'rb') as f:\n",
    "    word_to_index = pickle.load(f)\n",
    "with open(main_path + 'middle_data/' + 'pad_index_to_word.pkl', 'rb') as f:\n",
    "    index_to_word = pickle.load(f)\n",
    "vocab_size = len(word_to_index) + 1\n",
    "maxLen=20\n",
    "def get_file_list(file_path):\n",
    "    dir_list = os.listdir(file_path)\n",
    "    if not dir_list:\n",
    "        return\n",
    "    else:\n",
    "        dir_list = sorted(dir_list, key=lambda x: os.path.getmtime(os.path.join(file_path, x)))\n",
    "    return dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1572,
     "status": "ok",
     "timestamp": 1558422380363,
     "user": {
      "displayName": "王佳义",
      "photoUrl": "https://lh6.googleusercontent.com/-HjybI7giBHo/AAAAAAAAAAI/AAAAAAAAAAg/zzAUS8C4zH4/s64/photo.jpg",
      "userId": "03935660453405972628"
     },
     "user_tz": -480
    },
    "id": "Cmva_oZ735SI",
    "outputId": "1a6dd587-bd6a-49f9-819b-8d03e8d57922"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "def generate_train(batch_size):\n",
    "    print('\\n*********************************generate_train()*********************************')\n",
    "    steps=0\n",
    "    question_ = question\n",
    "    answer_ = answer\n",
    "    while True:\n",
    "        batch_answer_o = answer_o[steps:steps+batch_size]\n",
    "        batch_question = question_[steps:steps+batch_size]\n",
    "        batch_answer = answer_[steps:steps+batch_size]\n",
    "        outs = np.zeros([batch_size, maxLen, vocab_size], dtype='float32')\n",
    "        for pos, i in enumerate(batch_answer_o):\n",
    "            for pos_, j in enumerate(i):\n",
    "                if pos_ > 20:\n",
    "                    print(i)\n",
    "                outs[pos, pos_, j] = 1 # one-hot\n",
    "        yield [batch_question, batch_answer], outs\n",
    "        steps += batch_size\n",
    "        if steps == 100000:\n",
    "            steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "70MXGeDh4yxF"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.layers import Input, Dense, LSTM, TimeDistributed, Bidirectional, Dropout, Concatenate, RepeatVector, Activation, Dot\n",
    "from keras.layers import concatenate, dot                    \n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard,ReduceLROnPlateau\n",
    "from keras.initializers import TruncatedNormal\n",
    "import pydot\n",
    "import os, re\n",
    "truncatednormal = TruncatedNormal(mean=0.0, stddev=0.05)\n",
    "embed_layer = Embedding(input_dim=vocab_size, \n",
    "                        output_dim=100, \n",
    "                        mask_zero=True,\n",
    "                        input_length=None,\n",
    "                        embeddings_initializer= truncatednormal)\n",
    "# embed_layer.build((None,))\n",
    "\n",
    "LSTM_encoder = LSTM(512,\n",
    "                      return_sequences=True,\n",
    "                      return_state=True,\n",
    "#                       activation='relu',\n",
    "#                       dropout=0.25,\n",
    "#                       recurrent_dropout=0.1,\n",
    "                      kernel_initializer= 'lecun_uniform',\n",
    "                      name='encoder_lstm'\n",
    "                        )\n",
    "LSTM_decoder = LSTM(512, \n",
    "                    return_sequences=True, \n",
    "                    return_state=True, \n",
    "#                     activation = 'relu',\n",
    "#                     dropout=0.25, \n",
    "#                     recurrent_dropout=0.1,\n",
    "                    kernel_initializer= 'lecun_uniform',\n",
    "                    name='decoder_lstm'\n",
    "                   )\n",
    "\n",
    "#encoder输入 与 decoder输入\n",
    "input_question = Input(shape=(None, ), dtype='int32', name='input_question')\n",
    "input_answer = Input(shape=(None, ), dtype='int32', name='input_answer')\n",
    "\n",
    "input_question_embed = embed_layer(input_question)\n",
    "input_answer_embed = embed_layer(input_answer)\n",
    "\n",
    "\n",
    "encoder_lstm, question_h, question_c = LSTM_encoder(input_question_embed)\n",
    "\n",
    "decoder_lstm, _, _ = LSTM_decoder(input_answer_embed, \n",
    "                                  initial_state=[question_h, question_c])\n",
    "\n",
    "attention = dot([decoder_lstm, encoder_lstm], axes=[2, 2])\n",
    "attention = Activation('softmax')(attention)\n",
    "context = dot([attention, encoder_lstm], axes=[2,1])\n",
    "decoder_combined_context = concatenate([context, decoder_lstm])\n",
    "\n",
    "# output = dense1(decoder_combined_context)\n",
    "# output = dense2(Dropout(0.5)(output))\n",
    "\n",
    "# Has another weight + tanh layer as described in equation (5) of the paper\n",
    "decoder_dense1 = TimeDistributed(Dense(256,activation=\"tanh\"))\n",
    "decoder_dense2 = TimeDistributed(Dense(vocab_size,activation=\"softmax\"))\n",
    "output = decoder_dense1(decoder_combined_context) # equation (5) of the paper\n",
    "output = decoder_dense2(output) # equation (6) of the paper\n",
    "\n",
    "model = Model([input_question, input_answer], output)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "filepath = main_path + \"modles/W-\" + \"-{epoch:3d}-{loss:.4f}-.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath,\n",
    "                             monitor='loss',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='min',\n",
    "                             period=1,\n",
    "                             save_weights_only=True\n",
    "                             )\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', \n",
    "                              factor=0.2, \n",
    "                              patience=2, \n",
    "                              verbose=1, \n",
    "                              mode='min', \n",
    "                              min_delta=0.0001, \n",
    "                              cooldown=0, \n",
    "                              min_lr=0\n",
    "                              )\n",
    "tensorboard = TensorBoard(log_dir=main_path + 'logs', \n",
    "#                           histogram_freq=0, \n",
    "                          batch_size=100\n",
    "#                           write_graph=True, \n",
    "#                           write_grads=True, \n",
    "#                           write_images=True, \n",
    "#                           embeddings_freq=0, \n",
    "#                           embeddings_layer_names=None, \n",
    "#                           embeddings_metadata=None, \n",
    "#                           embeddings_data=None, \n",
    "#                           update_freq='epoch'\n",
    "                         )\n",
    "callbacks_list = [checkpoint, reduce_lr, tensorboard]\n",
    "\n",
    "initial_epoch_=0\n",
    "file_list = os.listdir(main_path + 'modles/')\n",
    "if len(file_list) > 0:\n",
    "    epoch_list = get_file_list(main_path + 'modles/')\n",
    "    epoch_last = epoch_list[-1]\n",
    "    model.load_weights(main_path + 'modles/' + epoch_last)\n",
    "    print(\"**********checkpoint_loaded: \", epoch_last)\n",
    "    initial_epoch_ = int(epoch_last.split('-')[2]) - 1\n",
    "    print('**********Begin from epoch: ', str(initial_epoch_))\n",
    "\n",
    "model.fit_generator(generate_train(batch_size=100), \n",
    "                    steps_per_epoch=1000, # (total samples) / batch_size 100000/100 = 1000\n",
    "                    epochs=200, \n",
    "                    verbose=1, \n",
    "                    callbacks=callbacks_list, \n",
    "#                     validation_data=generate_test(batch_size=100), \n",
    "#                     validation_steps=200, # 10000/100 = 100\n",
    "                    class_weight=None, \n",
    "                    max_queue_size=5, \n",
    "                    workers=1, \n",
    "                    use_multiprocessing=False, \n",
    "                    shuffle=False, \n",
    "                    initial_epoch=initial_epoch_\n",
    "                    )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gmiOjkRzLGI_"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x95onNF_JGKf"
   },
   "outputs": [],
   "source": [
    "!pwd\n",
    "!ls \"/content/drive/My Drive/Colab Notebooks/modles/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 864,
     "status": "ok",
     "timestamp": 1558444630072,
     "user": {
      "displayName": "王佳义",
      "photoUrl": "https://lh6.googleusercontent.com/-HjybI7giBHo/AAAAAAAAAAI/AAAAAAAAAAg/zzAUS8C4zH4/s64/photo.jpg",
      "userId": "03935660453405972628"
     },
     "user_tz": -480
    },
    "id": "lrjU1mL0dLGp",
    "outputId": "a002dcdc-fe9d-4e7d-bdb6-6370727c774f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '4.6182', 2: '3.3330', 3: '2.8432', 4: '2.5247', 5: '2.2923', 6: '2.1136', 7: '1.9717', 8: '1.8598', 9: '1.7741', 10: '1.7182', 11: '1.6781', 12: '1.6385', 13: '1.5959', 14: '1.5527', 15: '1.5120', 16: '1.4736', 17: '1.4388', 18: '1.4066', 19: '1.3768', 20: '1.3491', 21: '1.3245', 22: '1.3019', 23: '1.2808', 24: '1.2622', 25: '1.2450', 26: '1.2292', 27: '1.2156', 28: '1.2026', 29: '1.1908', 30: '1.1796', 31: '1.1694', 32: '1.1601', 33: '1.1517', 34: '1.1434', 35: '1.1358', 36: '1.1288', 37: '1.1222', 38: '1.1161', 39: '1.1102', 40: '1.1049', 41: '1.0993', 42: '1.0951', 43: '1.0901', 44: '1.0861', 45: '1.0816', 46: '1.0774', 47: '1.0735', 48: '1.0697', 49: '1.0660', 50: '1.0631', 51: '1.0604', 52: '1.0568', 53: '1.0541', 54: '1.0509', 55: '1.0483', 56: '1.0453', 57: '1.0430', 58: '1.0403', 59: '1.0377', 60: '1.0364', 61: '1.0330', 62: '1.0314', 63: '1.0297', 64: '1.0272', 65: '1.0254', 66: '1.0238', 67: '1.0219', 68: '1.0200', 69: '1.0185', 70: '1.0165', 71: '1.0151', 72: '1.0138', 73: '1.0121', 74: '1.0106', 75: '1.0092', 76: '1.0079', 77: '1.0064', 78: '1.0055', 79: '1.0042', 80: '1.0027', 81: '1.0015', 82: '0.9994', 83: '0.9981', 84: '0.9974', 85: '0.9969', 86: '0.9956', 87: '0.9951', 88: '0.9937', 89: '0.9928', 90: '0.9911', 91: '0.9905', 92: '0.9900', 93: '0.9888', 94: '0.9876', 95: '0.9871', 96: '0.9860', 97: '0.9850', 98: '0.9840', 99: '0.9835', 100: '0.9827', 101: '0.9824', 102: '0.9816', 103: '0.9809', 104: '0.9797', 105: '0.9796', 106: '0.9784', 107: '0.9775', 108: '0.9770', 109: '0.9759', 110: '0.9758', 111: '0.9753', 112: '0.9749', 113: '0.9742', 114: '0.9735', 115: '0.9724', 116: '0.9720', 117: '0.9891', 118: '0.9768', 119: '0.9735', 120: '0.9716', 121: '0.9705', 122: '0.9694', 123: '0.9687', 124: '0.9682', 125: '0.9851', 126: '0.9732', 127: '0.9692', 128: '0.9679', 129: '0.9658', 130: '0.9651', 131: '0.9641', 132: '0.9636', 133: '0.9635', 134: '0.9633', 135: '0.9628', 136: '0.9625', 137: '0.9623', 138: '0.9617', 139: '0.9611', 140: '0.9606', 141: '0.9601', 142: '0.9774', 143: '0.9654', 144: '0.9620', 145: '0.9596', 146: '0.9585', 148: '0.9581', 149: '0.9572', 150: '0.9571', 151: '0.9566', 152: '0.9562', 153: '0.9552', 156: '0.9194', 157: '0.9037', 158: '0.8971', 159: '0.8930', 160: '0.8901', 161: '0.8878', 162: '0.8858', 163: '0.8841', 164: '0.8825', 165: '0.8811', 166: '0.8797', 167: '0.8785', 168: '0.8774', 169: '0.8763', 170: '0.8753', 171: '0.8743', 172: '0.8734', 173: '0.8726', 174: '0.8718', 175: '0.8710', 176: '0.8702', 177: '0.8696', 178: '0.8688', 179: '0.8682', 180: '0.8676', 181: '0.8670', 182: '0.8664', 183: '0.8658', 184: '0.8653', 185: '0.8647', 186: '0.8642', 187: '0.8637', 188: '0.8632', 189: '0.8627', 190: '0.8622', 191: '0.8618', 192: '0.8614', 193: '0.8609', 194: '0.8605', 195: '0.8601', 196: '0.8597', 197: '0.8593', 198: '0.8589', 199: '0.8586', 200: '0.8582'}\n"
     ]
    }
   ],
   "source": [
    "import  pickle\n",
    "file_list = os.listdir(main_path + 'modles/')\n",
    "loss_list = {}\n",
    "for modelname in file_list:\n",
    "    loss_list[int(modelname.split('-')[2])] = modelname.split('-')[3]\n",
    "print(loss_list)\n",
    "# np.save(main_path + 'middle_data/loss_list.npy', loss_list)\n",
    "with open(main_path + 'middle_data/loss_list.npy', 'wb') as f:\n",
    "    pickle.dump(loss_list, f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "zh_chatbot_V2.ipynb",
   "provenance": [
    {
     "file_id": "1SwcGOX6iwpX4t0Lru4B67-saAFDhQ5x_",
     "timestamp": 1554991421755
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
